# Performance Testing

This repository contains resources that may be useful when performance testing the `Hedera-hcs-token-demo` project.

Performance testing is performed against the REST API layer, that is sending large quantities of requests to the REST API server, simulating a load of 1000’s of clients at the same time. A utility to provide end to end metrics is also provided, this sends REST API requests to the application and times the time taken for the operation to complete, thus simulating an end user's experience *Note: The sampling frequency of the end to end tester is 1 second which may give slightly higher latency results than actually provided by the application*.

## Testing for performance

### jMeter installation

JMeter has been used to generate API requests. It is recommended a separate server is setup for this purpose since generating heavy workloads can be CPU intensive.

We have used a c2-standard-8 (8 vCPUs, 32 GB memory) Google Compute virtual machine for our testing.

#### Install Java

```shell script
sudo apt-get update

sudo apt-get install default-jdk
```

> verify the java installation

```shell script
java -version
```

> output

```text
openjdk version "1.8.0_252"
OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~deb9u1-b09)
OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)
```

#### Install jMeter

```shell script
cd ~

wget -c http://www-eu.apache.org/dist//jmeter/binaries/apache-jmeter-5.3.zip

unzip apache-jmeter-5.3.zip
```

#### Create an alias for jMeter

Creating an alias makes it easier to run jMeter. If the unzipped jMeter files are located in `/home/test/apache-jmeter-5.3`

```shell script
nano ~/.bashrc
```

add

```text
alias jmeter='/home/test/apache-jmeter-5.3/bin/jmeter -n' 
```

#### Test the alias

```shell script
cd ~

jmeter -v
```

#### Install Git

```shell script
sudo apt update

sudo apt install git

git version
```

> Output

```text
git version 2.11.0
```

#### Download the testing source code from git

```shell script
cd ~

git clone https://github.com/hashgraph/hedera-stable-coin-demo
```

#### Compile the data generator

```shell script
cd hedera-stable-coin-demo/e2eTesting

./build.sh
```

### jMeter test data

The `~/hedera-stable-coin-demo/e2eTesting/stabl-test` folder contains a few JMeter scripts that can be used to perform tests on Join (`Join.jmx`), Buy (`Buy.jmx`), Redeem (`Burn.jmx`), Send (`Send.jmx`) and a mixture (`Mix.jmx`) of operations.

Each of these scripts takes data from csv files and runs 10 concurrent load processes.
When running jMeter it’s possible to specify how many threads each process should run in parallel, along with the number of loops (or rows) each thread in each process should process.

Calculations: 
* 10 x threads x loops = number of operations generated by the scripts.
* threads x loops = number of csv rows required

*Note: it is essential that the csv files contain at least as many operations as specified by the above calculation, otherwise, the threads will stop at the end of the file leading to a lower number of operations than expected.*

*Note: the `send` csv files will contain 10 times as many rows as the others to support the `Mix.jmx` script testing.

#### JMeter csv file generation

In order to successfully run the jMeter tests, a number of CSV files need to be prepared in advance. These contain data that is used by each of the tests to create a JSON payload to send to the REST API.

A utility is provided to build these files, when the utility is run, any csv files in the stabl-test folder are replaced with the newly generated files.

The utility takes an optional command line parameter to specify the number of lines each csv file should contain (the greater the number of lines, the longer it will take to generate the files). If no parameter is specified, the utility will generate 1 million rows in each file (10 million in send csv files).

*Note: The number of lines in the files doesn’t drive the quantity of data sent to the REST API, this is specified through the threads and loops parameters of the jMeter application. However care must be taken to ensure the files contain sufficient rows.*

The utility generates four types of files, `join_x.csv`, `buy_x.csv`, `send_x.csv` and `burn_x.csv`.
For each type of file, ten files are created, they are named `join_a.csv`, `join_b.csv`, …, `join_j.csv` (likewise for the other file types). Each process within the jMeter `.jmx` files uses one of these files (the `Mix.jmx` uses join, buy and send).

Consequently, if each file contains 1M rows, there is enough data in 10 files to generate 10M operations in a single script run.

***Join_x.csv***

These files contain a public key and a username. The public key is random (running the utility twice will yield different public keys), the username is x_user_yyyyy where x is either a, b, c… j and yyyy is the csv row number (e.g. a_user_13).

***Buy_x.csv***

These files contain a username (x_user_yyyyy) which is from the same list as join_x.csv files, and a random amount to purchase.

***Send_x.csv***

These files contain a base64 encoded signed operation. Inside the operation, the user identified by the csv row number and file id (e.g. a_user_1000) sends a random amount of STABL token to a random user from the join list for the same file id (e.g. a_user_1004 but not c_user_1004 if the file is send_a.csv).
*Note: it’s possible due to randomness that the user tries to send more than its balance allows.*

***Burn_x.csv***

These files contain a base64 encoded signed operation. Inside the operation, the user identified by the csv row number burns a random amount of STABL token.
*Note: it’s possible due to randomness that the user tries to burn more than its balance allows.*

***To generate files, run the following command***

```shell script
cd ~/hedera-hcs-token-go-perf-test

java -jar runme.jar [optional number of lines]
```

### Running JMeter tests

Before running the tests, ensure the Database, Mirror Subscriber and REST API components are up and running.

Below is an example command for running each of the possible four tests

Note, the command takes the IP address and Port of the server running the REST API. The IP and port must be open through any firewall that may exist between the two servers.

The command also takes the number of threads per process (default 5), loops (default 10) per thread and target transactions per second (default 1000) you wish to run.

Note: 50 threads should be capable of 3500+ TPS

```shell script
cd ~/hedera-stable-coin-demo/e2eTesting/stabl-test

JVM_ARGS='-Xms2048m -Xmx2048m' jmeter -t Join.jmx -j meter.log -Jip=RESTIP -Jport=RESTPORT -Jloops=10 -Jthreads=50 -Jtps=1000

JVM_ARGS='-Xms2048m -Xmx2048m' jmeter -t Buy.jmx -j meter.log -Jip=RESTIP -Jport=RESTPORT -Jloops=10 -Jthreads=50 -Jtps=1000

JVM_ARGS='-Xms2048m -Xmx2048m' jmeter -t Send.jmx -j meter.log -Jip=RESTIP -Jport=RESTPORT -Jloops=10 -Jthreads=50 -Jtps=1000

JVM_ARGS='-Xms2048m -Xmx2048m' jmeter -t Burn.jmx -j meter.log -Jip=RESTIP -Jport=RESTPORT -Jloops=10 -Jthreads=50 -Jtps=1000

JVM_ARGS='-Xms2048m -Xmx2048m' jmeter -t Mix.jmx -j meter.log -Jip=RESTIP -Jport=RESTPORT -Jloops=10 -Jloops_send=100 -Jthreads=5 -Jthreads_send=50 -Jtps=1000
```

(The above are examples only, higher TPS might require a greater number of overall threads.)

*Note the additional parameters on the last `Mix.jmx` test, `loops_send` and `threads_send` set the number of loops and threads for transfer operations independently of `join` and `buy` operations, this enables you to create a greater number of transfer operations than join and buy if desired.

It is best to run `Join`, then `Buy`. From there `Send` or `Burn` can be run. Indeed, running `Send` just after `Join` will result in 100% failures since the STABL accounts are initialised with 0 STABL tokens.
Or, running `Mix` will run a mixture of all operations (except `Burn`).

While the test is running, the mirror subscriber outputs performance statistics.
*Note: these are reset every time the subscriber is restarted and output every 2000 operations.*

| Heading   | Description        |
|-----------|--------------------|
| Secs      | The number of seconds since the first notification received |
| Count     | The number of operations notified to the subscriber |
| AvgTPS    | The above Count / Secs = average transactions per second processed since the mirror subscriber was started |
| BlockTPS  | The transactions per second for the last block of 2000 operations | 
| BlockSecs | The number of seconds spent processing the last block of 2000 operations | 
	 
*Note: if the mirror subscriber is left running between two test runs, the AvgTPS and the BlockTPS will reflect the time the application was idle, for more accurate averages, restart the subscriber before launching a new test.*

### Repeating jMeter tests

Running the same test csv files will result in a 100% failure rate since the operations would be identified as duplicates. It is therefore necessary to delete the database data prior to running tests with the same files.

*Note: A backup of the database (e.g. `sudo pg_dump -h localhost -U postgres -W  > postgresBackup.sql`) after a `join` and `buy` test is recommended so you don't have to repeat it over and over again prior to re-running `buy` and `send` or `burn` respectively.

To do so, from the database server prompt:

```shell script
sudo -u postgres psql

drop table _migrations;
drop table address;
drop table operation;

CTRL+D
```

Restart the mirror and REST processes to ensure the database entities are re-created.

You may now restart the tests.

### JMeter log files

Running the tests will generate several log files for subsequent analysis.

`meter.log` (likely located in `stabl-test`) contains a summary of the jMeter test run

The following files will contain operations that failed (response 500 from the api) and are likely located in the `bin` folder of your jMeter installation but may be found in the `stabl-test` folder.

* `errors-join.log` 
* `errors-buy.log`
* `errors-send.log`
* `errors-burn.log`
* `errors-mix.log`

## Testing for latency

If you haven't already compiled the data generator earlier

```shell script
cd hedera-stable-coin-demo/e2eTesting

./build.sh
```

then 

```shell script
java -jar runme.jar e2e rest_server_ip:3128 mirror_server_ip:3129
```

the latency tester will start and run in an infinite loop. `CTRL+C` will stop it.

it will output the following to the console and also generate a file.

### Console output

The tester first runs a `join` operation followed by a `buy (MintTo)` operation. Subsequent tests include a `Join`, `MintTo` and `Transfer` to a random user created by the earlier tests.

> Output

```text
2020-07-01T12:36:39.952034Z Join api (2), complete (12), user api get call (0), queryCount (8)
2020-07-01T12:36:51.228640Z MintTo api (0), complete (9), user api get call (0), queryCount (8)

2020-07-01T12:37:00.877270Z Join api (1), complete (10), user api get call (0), queryCount (8)
2020-07-01T12:37:10.412215Z MintTo api (0), complete (9), user api get call (0), queryCount (8)
2020-07-01T12:37:19.950793Z Transfer api (1), complete (11), user api get call (1), queryCount (8)
```

It outputs the start time of each operation in UTC, followed by the api call it made (Join, MintTo or Transfer) and the time this API call took `(1)` in seconds.

It then queries the application to check if:
* Join: The user exists
* MintTo: The user's balance is equal to the initial balance specified in the MintTo request
* Transfer: The user's balance is different to the initial MintTo amount

Once the above condition is met, it outputs `complete (x)` where `x` is the total time taken from start to finish which is the overall latency of the operation.
Finally it outputs the time taken for the successful query api in seconds and the number of queries required to reach success.
*Note: A timeout of 30s on the queries is in place should the server not respond within that time to allow testing to continue*

Once the first cycle of `Join + MintTo` then `Join + MintTo + Send` are complete, aggregate data is also output to the console

> output

```text
Join Min=10, Max=12, 0-10=1, 11-15=1, 16-20=0, 20+=0
Mint Min=9, Max=9, 0-10=2, 11-15=0, 16-20=0, 20+=0
Send Min=11, Max=11, 0-10=0, 11-15=1, 16-20=0, 20+=0
```

This shows the `min` and `max` latency for each type of operation and the number of operations that fell in the `0-10 seconds`, `11-15 seconds`, `16-20 seconds` and `20+ seconds`.
*Note: The ranges are inclusive, so `0-10` contains all latencies that were `>=0 and <=10`

### File output

A file is also generated with the results of the test, the file is named `1593599057-log.txt` where the digits represent the number of seconds since epoch at the time the test was started.

The file contains recurring aggregate data from each of the tests with each batch of data adding to the previous one.

> example

```text
Operation, Min, Max, 0-10 count, 0-10 average, 11-15 count, 11-15 average, 16-20 count, 16-20 average, 20- count, 20- average
Join     , 10 , 11 , 2         , 10.0        , 1          , 11.0         , 0          ,              , 0        , 0          
Mint     , 8  , 10 , 3         , 9.0         , 0          ,              , 0          ,              , 0        , 0          
Send     , 8  , 9  , 2         , 8.5         , 0          ,              , 0          ,              , 0        , 0          
Summary  , 8  , 11 , 7         , 9.14        , 1          , 11.00        , 0          , 0.00         , 0        , 0.00       
Overall average: 9.38
API Timeouts (>10): Join=0, Buy=0, Send=0
```

We find the same `min`, `max` and totals for each time range as well as an average for each of the time ranges.
An overall average including all time ranges is finally output as well as any recorded API call timeouts.